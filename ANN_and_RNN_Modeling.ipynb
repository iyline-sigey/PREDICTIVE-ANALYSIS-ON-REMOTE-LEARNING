{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN and RNN Modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iyline-sigey/PREDICTIVE-ANALYSIS-ON-REMOTE-LEARNING/blob/Modelling/ANN_and_RNN_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxUb7m7XW8Od"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#Libraries for modelling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import regularizers\n",
        "from keras import layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlOmZ4o9eLlR"
      },
      "source": [
        "**Load the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMuDDdWBcis5"
      },
      "source": [
        "df=pd.read_csv('modeling_data.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj68cMjdzMfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4176ede-148b-45a7-da67-22075bef6bdc"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'clean_tweet', 'class'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4uZDloszPGc"
      },
      "source": [
        "df.drop(['Unnamed: 0'], axis =1, inplace=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7ZJ7oY1zWJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39dcca85-aeeb-43b0-a570-0e3a5c619168"
      },
      "source": [
        "#Checking the shape of the minority class\n",
        "a= df[df['class']==0]\n",
        "a.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PonZsuT8zlFv"
      },
      "source": [
        "Dealing with the class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slxORJQrzhNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7822b2a-29cc-417a-bdf1-4f58ba186aee"
      },
      "source": [
        "#We will use up-sampling to solve the clase imbalance\n",
        "#Up-sampling is the process of randomly duplicating observations from the minority class in order to reinforce its signal\n",
        "\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Separate majority and minority classes\n",
        "df_majority = df[df['class']==1]\n",
        "df_minority = df[df['class']==0]\n",
        " \n",
        "# Upsample minority class\n",
        "df_minority_upsampled = resample(df_minority, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=130,    # to match majority class\n",
        "                                 random_state=0) # reproducible results\n",
        " \n",
        "# Combine majority class with upsampled minority class\n",
        "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
        " \n",
        "# Display new class counts\n",
        "df_upsampled['class'].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    192\n",
              "0    130\n",
              "Name: class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BgNEXCMTHMT"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt1k3pWDVsAi"
      },
      "source": [
        "**Splitting the Dataset into Train and Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ONNqOScVq5-",
        "outputId": "5fd0d3e7-aec1-4858-f155-f57231a5a74f"
      },
      "source": [
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=0)\n",
        "\n",
        "print('df_train shape: {}'.format(df_train.shape))\n",
        "print('df_test shape: {}'.format(df_test.shape))\n",
        "\n",
        "print('df_train: {:.2f}% positive reviews'.format(df_train['class'].mean()*100))\n",
        "print('df_test: {:.2f}% positive reviews'.format(df_test['class'].mean()*100))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df_train shape: (248, 2)\n",
            "df_test shape: (62, 2)\n",
            "df_train: 60.89% positive reviews\n",
            "df_test: 66.13% positive reviews\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpT8eu9RWJAJ"
      },
      "source": [
        "**Further Splitting the Train dataset into Train and Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94SL8G1dWER7"
      },
      "source": [
        "# Split the data into train and validation set.\n",
        "df0_train, df0_val = train_test_split(df_train, test_size=0.2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v30j_E6VWQHH"
      },
      "source": [
        "#set the target and features\n",
        "X_train = df0_train['clean_tweet'].values\n",
        "y_train = df0_train['class'].values\n",
        "\n",
        "X_val = df0_val['clean_tweet'].values\n",
        "y_val = df0_val['class'].values"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdzvCnbZWmkv"
      },
      "source": [
        "**Preprocessing the Text: Tokenization and Conversion to Sequences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGeZQJ8UWQEN"
      },
      "source": [
        "# set a vocabulary size. This is the maximum number of words that can be used.\n",
        "vocabulary_size = 10000\n",
        "\n",
        "# create the tokenizer that comes with Keras.\n",
        "tokenizer = Tokenizer(num_words=vocabulary_size)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# convert the texts to sequences.\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_val_seq = tokenizer.texts_to_sequences(X_val)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57u-cNuxWPrA",
        "outputId": "a72e0d96-f404-42db-8035-a1c65fc77eb2"
      },
      "source": [
        "l = [len(i) for i in X_train_seq]\n",
        "l = np.array(l)\n",
        "print('minimum number of words: {}'.format(l.min()))\n",
        "print('median number of words: {}'.format(np.median(l)))\n",
        "print('average number of words: {}'.format(l.mean()))\n",
        "print('maximum number of words: {}'.format(l.max()))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "minimum number of words: 4\n",
            "median number of words: 22.0\n",
            "average number of words: 22.252525252525253\n",
            "maximum number of words: 46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM3or56EzxIK"
      },
      "source": [
        "X_train_seq_padded = pad_sequences(X_train_seq, maxlen=200)\n",
        "X_val_seq_padded  = pad_sequences(X_val_seq, maxlen=200)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpAipZBZyzka"
      },
      "source": [
        "**ANN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqTNmNMGxmjb",
        "outputId": "527c44d0-2354-4367-e73f-b82c0980d221"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#Create a tensorflow model\n",
        "embedding_vector_length = 32\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "model = tf.keras.models.Sequential() \n",
        "model.add(tf.keras.layers.Embedding(vocab_size, embedding_vector_length, input_length=200) )\n",
        "model.add(tf.keras.layers.SpatialDropout1D(0.25))\n",
        "model.add(tf.keras.layers.LSTM(50, dropout=0.5, recurrent_dropout=0.5))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid')) \n",
        "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])  \n",
        "print(model.summary())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 200, 32)           37216     \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d (SpatialDr (None, 200, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 50)                16600     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 53,867\n",
            "Trainable params: 53,867\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24i0R8xI1Cx3",
        "outputId": "37876a2e-6025-4a41-99a1-f275f7193cce"
      },
      "source": [
        "#fit the model\n",
        "history = model.fit(X_train_seq_padded,y_train,validation_split=0.2, epochs=5, batch_size=32)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "5/5 [==============================] - 5s 379ms/step - loss: 0.6918 - accuracy: 0.5633 - val_loss: 0.6884 - val_accuracy: 0.6250\n",
            "Epoch 2/5\n",
            "5/5 [==============================] - 1s 276ms/step - loss: 0.6867 - accuracy: 0.5823 - val_loss: 0.6780 - val_accuracy: 0.6250\n",
            "Epoch 3/5\n",
            "5/5 [==============================] - 1s 275ms/step - loss: 0.6749 - accuracy: 0.5886 - val_loss: 0.6595 - val_accuracy: 0.6250\n",
            "Epoch 4/5\n",
            "5/5 [==============================] - 1s 280ms/step - loss: 0.6551 - accuracy: 0.5886 - val_loss: 0.6275 - val_accuracy: 0.6250\n",
            "Epoch 5/5\n",
            "5/5 [==============================] - 1s 281ms/step - loss: 0.6478 - accuracy: 0.5886 - val_loss: 0.6087 - val_accuracy: 0.6250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG2Y-mKl1XbG"
      },
      "source": [
        "**RNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyPVt2ZlBnqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e81d4040-ad6b-4cc1-c324-6a92f4a5f4b2"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "max_words = 5000\n",
        "max_len = 200\n",
        "\n",
        "X_train_seq_padded = pad_sequences(X_train_seq, maxlen=200)\n",
        "X_val_seq_padded  = pad_sequences(X_val_seq, maxlen=200)\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(layers.Embedding(max_words, 20)) #The embedding layer\n",
        "model1.add(layers.LSTM(15,dropout=0.5)) #Our LSTM layer\n",
        "model1.add(layers.Dense(1,activation='sigmoid'))\n",
        "print(model1.summary())\n",
        "\n",
        "model1.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "checkpoint1 = ModelCheckpoint(\"best_model1.hdf5\", monitor='val_accuracy', verbose=1,\n",
        "                              save_best_only=True,mode='auto', period=1,save_weights_only=False)\n",
        "#model.fit(X_train, Y_train,validation_data = (X_test,y_test),epochs = 10, batch_size=32)\n",
        "\n",
        "history = model1.fit(X_train_seq_padded, y_train, epochs=10,batch_size=32,\n",
        "                     validation_data=(X_val_seq_padded, y_val),callbacks=[checkpoint1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 20)          100000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 15)                2160      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 102,176\n",
            "Trainable params: 102,176\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/10\n",
            "7/7 [==============================] - 4s 214ms/step - loss: 0.6864 - accuracy: 0.5808 - val_loss: 0.6656 - val_accuracy: 0.6600\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.66000, saving model to best_model1.hdf5\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 1s 90ms/step - loss: 0.6606 - accuracy: 0.5960 - val_loss: 0.6222 - val_accuracy: 0.6600\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.66000\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 1s 92ms/step - loss: 0.6335 - accuracy: 0.5960 - val_loss: 0.5963 - val_accuracy: 0.6600\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.66000\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 1s 94ms/step - loss: 0.5945 - accuracy: 0.6010 - val_loss: 0.5514 - val_accuracy: 0.6600\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.66000\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 1s 90ms/step - loss: 0.5431 - accuracy: 0.6616 - val_loss: 0.5257 - val_accuracy: 0.8800\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.66000 to 0.88000, saving model to best_model1.hdf5\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 1s 91ms/step - loss: 0.5215 - accuracy: 0.7475 - val_loss: 0.4577 - val_accuracy: 0.7600\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88000\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 1s 92ms/step - loss: 0.4543 - accuracy: 0.7879 - val_loss: 0.4257 - val_accuracy: 0.7200\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88000\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 1s 95ms/step - loss: 0.4156 - accuracy: 0.8586 - val_loss: 0.3803 - val_accuracy: 0.8800\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88000\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 1s 92ms/step - loss: 0.3852 - accuracy: 0.8838 - val_loss: 0.3941 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.88000 to 0.96000, saving model to best_model1.hdf5\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 1s 94ms/step - loss: 0.3369 - accuracy: 0.9545 - val_loss: 0.3223 - val_accuracy: 0.9000\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.96000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA83mDDw1ZX3",
        "outputId": "5e89bc5e-2f2c-4b5c-a332-e5ecbbf26477"
      },
      "source": [
        "#Evaluating the model using the test data\n",
        "# predict on the test dataset.\n",
        "\n",
        "# transform text to sequences.\n",
        "X_test_seq = tokenizer.texts_to_sequences(df_test['clean_tweet'].values)\n",
        "X_test_seq_padded = pad_sequences(X_test_seq, maxlen=200)\n",
        "y_test = df_test['class'].values\n",
        "\n",
        "y_test_pred = model1.predict(X_test_seq_padded)\n",
        "y_test_pred = y_test_pred.reshape(y_test_pred.shape[0],)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "auc_lstm = roc_auc_score(y_test, y_test_pred)\n",
        "auc_lstm"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1qOKsJZlGM_"
      },
      "source": [
        "**Bidirectional layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0mIv3FdL00S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e412ae2-d852-4e23-ff39-2e4ec633d537"
      },
      "source": [
        "#Intialize the model\n",
        "model2 = Sequential()\n",
        "model2.add(layers.Embedding(max_words, 40, input_length=max_len))\n",
        "model2.add(layers.Bidirectional(layers.LSTM(20,dropout=0.6)))\n",
        "model2.add(layers.Dense(1,activation='sigmoid'))\n",
        "#Call comipiler ab=nd the checkpoints\n",
        "\n",
        "model2.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['accuracy'])\n",
        "checkpoint2 = ModelCheckpoint(\"best_model2.hdf5\", monitor='val_accuracy', \n",
        "                              verbose=1,save_best_only=True, mode='auto', \n",
        "                              period=1,save_weights_only=False)\n",
        "\n",
        "#fit the model\n",
        "\n",
        "history = model2.fit(X_train_seq_padded, y_train, epochs=10,\n",
        "                     validation_data=(X_val_seq_padded, y_val),callbacks=[checkpoint2])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/10\n",
            "7/7 [==============================] - 6s 282ms/step - loss: 0.6847 - accuracy: 0.5909 - val_loss: 0.6613 - val_accuracy: 0.6600\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.66000, saving model to best_model2.hdf5\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 1s 146ms/step - loss: 0.6646 - accuracy: 0.5960 - val_loss: 0.6220 - val_accuracy: 0.6600\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.66000\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 1s 142ms/step - loss: 0.6285 - accuracy: 0.5960 - val_loss: 0.5970 - val_accuracy: 0.6600\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.66000\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 1s 139ms/step - loss: 0.5751 - accuracy: 0.6162 - val_loss: 0.5330 - val_accuracy: 0.6600\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.66000\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 1s 144ms/step - loss: 0.5070 - accuracy: 0.7626 - val_loss: 0.4634 - val_accuracy: 0.6600\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.66000\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 1s 143ms/step - loss: 0.4521 - accuracy: 0.8535 - val_loss: 0.4245 - val_accuracy: 0.7000\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.66000 to 0.70000, saving model to best_model2.hdf5\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 1s 139ms/step - loss: 0.3866 - accuracy: 0.9141 - val_loss: 0.3904 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.70000 to 1.00000, saving model to best_model2.hdf5\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 1s 140ms/step - loss: 0.3382 - accuracy: 0.9798 - val_loss: 0.3164 - val_accuracy: 0.9200\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 1.00000\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 1s 142ms/step - loss: 0.2865 - accuracy: 0.9646 - val_loss: 0.2701 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 1.00000\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 1s 144ms/step - loss: 0.2484 - accuracy: 0.9899 - val_loss: 0.2262 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 1.00000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u6HpQ6ippGV",
        "outputId": "ff4e632d-87c5-484d-fcdf-c5c4a30bab60"
      },
      "source": [
        "#Evaluating the model using the test data\n",
        "# predict on the test dataset.\n",
        "\n",
        "# transform text to sequences.\n",
        "X_test_seq = tokenizer.texts_to_sequences(df_test['clean_tweet'].values)\n",
        "X_test_seq_padded = pad_sequences(X_test_seq, maxlen=200)\n",
        "y_test = df_test['class'].values\n",
        "\n",
        "y_test_pred = model2.predict(X_test_seq_padded)\n",
        "y_test_pred = y_test_pred.reshape(y_test_pred.shape[0],)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "auc_lstm = roc_auc_score(y_test, y_test_pred)\n",
        "auc_lstm"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}